name: build-and-deploy-frontend

trigger:
  branches:
    include:
      - main  
  paths:
    include:
      - my-app/frontend/*

variables:
  - group: aws-keys
  - name: AWS_REGION
    value: us-east-2 # This value was modified by the initial-setup python script
  - name: APP_NAME
    value: mocha # This value was modified by the initial-setup python script
  - name: DOCKERHUB_USERNAME
    value: tferrari92 # This value was modified by the initial-setup python script
  - name: USER_EMAIL
    value: tomas.ferrari@sendati.com # This value was modified by the initial-setup python script
    
pool:
  vmImage: 'ubuntu-latest' 
  # If you are using a self-hosted agent, comment out the previous line and uncomment the following three
  # name: <agent-pool-name> # Insert here the name of the agent pool you created
  # demands:
  #   - agent.name -equals <agent-name> # Insert here the name of the agent you created

stages:
- stage: BuildApp
  jobs:
  - job: BuildJob
    displayName: 'Build Job'
    steps:
    - task: DockerInstaller@0
      displayName: Install Docker
      inputs:
        dockerVersion: '17.09.0-ce'

    - task: Docker@2
      displayName: Build And Push Image
      inputs:
        containerRegistry: 'dockerhub'
        repository: '$(DOCKERHUB_USERNAME)/$(APP_NAME)-frontend'
        command: 'buildAndPush'
        Dockerfile: 'my-app/frontend/Dockerfile'



- stage: DeployDev
  condition: succeeded()
  jobs:
  - job: DeployDevJob
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true

    # We update the local repo with a pull just in case there have been any recent modifications
    - script: |
        git pull origin main
      displayName: Update repo

    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/frontend/environments/values-dev.yaml > helm/my-app/frontend/environments/values-dev.temp
        mv helm/my-app/frontend/environments/values-dev.temp helm/my-app/frontend/environments/values-dev.yaml
      displayName: Update Tag In values-dev.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
        git checkout -b main
        git add helm/my-app/frontend/environments/values-dev.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    # - script: |
    #     echo "false" > is-active.txt
    #     while [ "$(cat is-active.txt)" != "true" ]; do
    #         sleep 5
    #         aws elbv2 describe-load-balancers > lb-status.json
    #         json_data=$(cat lb-status.json)
    #         state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
    #         if [[ "$state_code" == "active" ]]; then
    #           echo "true" > is-active.txt
    #         else
    #           echo "Load balancer is not ready yet..."
    #         fi
    #     done
    #   displayName: Wait for Load Balancer Status

    - task: AWSCLI@1
      displayName: Update KubeConfig
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME)-dev $(kubectl get ingress -n $(APP_NAME)-dev | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: Save URL

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'DEV-URL'
        publishLocation: 'Container'
        



- stage: DeployStage
  condition: succeeded()
  # dependsOn: DeployDev
  jobs:
  - job: DeployStageJob
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true
      #clean: true
  
    # Every time the pipeline performs a "checkout" step it will checkout the same commit, so we need this step to pull for the changes commited in the previous stage   
    - script: |
        git pull origin main
      displayName: Update repo
      
    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/frontend/environments/values-stage.yaml > helm/my-app/frontend/environments/values-stage.temp
        mv helm/my-app/frontend/environments/values-stage.temp helm/my-app/frontend/environments/values-stage.yaml
      displayName: Update Tag In values-stage.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
        git checkout -b main
        git add helm/my-app/frontend/environments/values-stage.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    # - script: |
    #     echo "false" > is-active.txt
    #     while [ "$(cat is-active.txt)" != "true" ]; do
    #         sleep 5
    #         aws elbv2 describe-load-balancers > lb-status.json
    #         json_data=$(cat lb-status.json)
    #         state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
    #         if [[ "$state_code" == "active" ]]; then
    #           echo "true" > is-active.txt
    #         else
    #           echo "Load balancer is not ready yet..."
    #         fi
    #     done
    #   displayName: 'Wait for Load Balancer Status'

    - task: AWSCLI@1
      displayName: 'Update KubeConfig'
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME)-stage $(kubectl get ingress -n $(APP_NAME)-stage | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: 'Save URL'

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'STAGE-URL'
        publishLocation: 'Container' 
        



- stage: DeployProd
  condition: succeeded()
  # dependsOn: DeployStage
  jobs:
  - job: ApproveRelease
    timeoutInMinutes: 4320 # Job times out in 3 days
    pool: server
    steps:
    - task: ManualValidation@0
      timeoutInMinutes: 1440 # Task times out in 1 day
      inputs:
        notifyUsers: '$(USER_EMAIL)'
        instructions: 'Please validate and approve deployment to prod.'

  - job: DeployProdJob
    dependsOn: ApproveRelease
    steps:

    # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
    - checkout: self
      persistCredentials: true
      #clean: true
      
    # Every time the pipeline performs a "checkout" step it will checkout the same commit, so we need this step to pull for the changes commited in the previous stage   
    - script: |
        git pull origin main
      displayName: Update repo

    - script: |
        sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/frontend/environments/values-prod.yaml > helm/my-app/frontend/environments/values-prod.temp
        mv helm/my-app/frontend/environments/values-prod.temp helm/my-app/frontend/environments/values-prod.yaml
      displayName: Update Tag In values-prod.yaml

    - script: |
        git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
        git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
        git checkout -b main
        git add helm/my-app/frontend/environments/values-prod.yaml 
        git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
        git push --set-upstream origin main
      displayName: 'Push changes to GitHub'

    - script: |
        mkdir ~/.aws
        echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
        echo -e "[default]\nregion = $(AWS_REGION)"> ~/.aws/config 
      displayName: 'Configure AWS Profile'

    # - script: |
    #     echo "false" > is-active.txt
    #     while [ "$(cat is-active.txt)" != "true" ]; do
    #         sleep 5
    #         aws elbv2 describe-load-balancers > lb-status.json
    #         json_data=$(cat lb-status.json)
    #         state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
    #         if [[ "$state_code" == "active" ]]; then
    #           echo "true" > is-active.txt
    #         else
    #           echo "Load balancer is not ready yet..."
    #         fi
    #     done
    #   displayName: 'Wait for Load Balancer Status'

    - task: AWSCLI@1
      displayName: 'Update KubeConfig'
      inputs:
        awsCredentials: 'aws'
        regionName: '$(AWS_REGION)' 
        awsCommand: 'eks'
        awsSubCommand: 'update-kubeconfig'
        awsArguments: '--name $(APP_NAME)-cluster --region $(AWS_REGION)' 

    - script: |
        kubectl get ingress -n $(APP_NAME)-prod $(kubectl get ingress -n $(APP_NAME)-prod | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
      displayName: 'Save URL'

    - task: PublishBuildArtifacts@1
      displayName: 'Export URL'
      inputs:
        PathtoPublish: 'my-app-url.txt'
        ArtifactName: 'PROD-URL'
        publishLocation: 'Container'
